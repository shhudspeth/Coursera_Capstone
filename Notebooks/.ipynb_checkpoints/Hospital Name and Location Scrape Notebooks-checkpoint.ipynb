{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import bs4\n",
    "import re\n",
    "import json\n",
    "import geocoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UTILITY FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A beautiful soup web scraping function\n",
    "def run_bs4(link, lxml=None):\n",
    "    user_agent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.122 Safari/537.36'\n",
    "    headers = {'User-Agent': user_agent}\n",
    "    page = requests.get(link)\n",
    "    html = page.content\n",
    "    soup = bs4.BeautifulSoup(html, 'html.parser')\n",
    "    return(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a regex function to find hospital names off a wiki link page \n",
    "def find_names(list_):\n",
    "    paragraph = ''\n",
    "    for x in list_:\n",
    "        #print(x)\n",
    "        try:\n",
    "            paragraph += str(x).replace('%27',\"\") + ' '\n",
    "        except:\n",
    "            pass\n",
    "    return(re.findall(r'/wiki/([\\S]+)', paragraph))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CODE TO GET HOSPITAL NAMES AND GEOLOCATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wget the wiki page of hospitals in the Bay area\n",
    "!wget https://en.wikipedia.org/wiki/Category:Hospitals_in_the_San_Francisco_Bay_Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run soup and other scrape functions to get a list of hospital names in the Bay Area\n",
    "soup = ' '\n",
    "with open('Category:Hospitals_in_the_San_Francisco_Bay_Area', 'r') as data_file:\n",
    "          for x in data_file.readlines():\n",
    "            soup += x \n",
    "\n",
    "soup_p = bs4.BeautifulSoup(soup)\n",
    "\n",
    "list_of_hospitals = []\n",
    "category = []\n",
    "\n",
    "# gets links to Oakland and SF City hospitals\n",
    "for x in soup_p.find_all('a'):\n",
    "    if 'Categories:' in str(x.get('href')):\n",
    "        category.append(str(x.get('href')))\n",
    "    if ':' in str(x.get('href')):\n",
    "        pass\n",
    "    elif '.org' in str(x.get('href')):\n",
    "        pass\n",
    "    elif 'Main' in str(x.get('href')):\n",
    "        pass\n",
    "    else:\n",
    "        list_of_hospitals.append(x.get('href'))\n",
    "\n",
    "# creates soup links for oakland and SFCity hospitals\n",
    "hospt_ = []\n",
    "for x in category[1:3]:\n",
    "    print(x)\n",
    "    new_link = 'https://en.wikipedia.org/' + x\n",
    "    hospt_.append(new_link)\n",
    "    \n",
    "# scrapes hospital names off oakland and SF city wiki pages  \n",
    "other_hospt = []\n",
    "for link in hospt_:\n",
    "    soup_p = run_bs4(link)\n",
    "    for x in soup_p.find_all('a'):\n",
    "        if ':' in str(x.get('href')):\n",
    "            pass\n",
    "        elif '.org' in str(x.get('href')):\n",
    "            pass\n",
    "        elif 'Main' in str(x.get('href')):\n",
    "            pass\n",
    "        else:\n",
    "            other_hospt.append(x.get('href'))\n",
    "            \n",
    "\n",
    "# Runs regex function to get just the names of hospitals (THE WIKI PAGE WAS A PAGE OF LINKS)\n",
    "hospital_names = find_names(list_of_hospitals[6:])\n",
    "other_names = find_names(other_hospt[3:])\n",
    "names = hospital_names + other_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GEOCODER FUNCTIONS TO GET LAT AND LONG OF HOSPITALS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hospital_loc = {}\n",
    "for hosp in names:\n",
    "    hosp = hosp.replace('_', \" \")\n",
    "    url = 'https://maps.googleapis.com/maps/api/geocode/json?'\n",
    "    params = {'address': hosp + 'California', 'key' :google_api_key}\n",
    "    r = requests.get(url, params=params)\n",
    "    results = r.json()\n",
    "    hospital_loc[hosp] = {'name' : hosp, 'lat': results['results'][0]['geometry']['location']['lat'], \\\n",
    "                         'lng': results['results'][0]['geometry']['location']['lng']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MAKES A CSV OF HOSPITAL NAMES WITH LAT AND LONG \n",
    "df_hos = pd.DataFrame(hospital_loc).T\n",
    "df_hos.to_csv('Hospital_locations.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
